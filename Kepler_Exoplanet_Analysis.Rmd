
Data Science Project
title of dataset: "Kepler Exoplanet Search Results"
source: Kaggle -  https://www.kaggle.com/nasa/kepler-exoplanet-search-results 
authors: Matthew Bazzo, Soo Hyung Choe, Shiming Yan, Alex Zhang 


# Introduction

The following dataset features 9564 observations, and 50 variables (12 categorical and 38 numerical), totally over 478000 data points. Some of the key variables are: disposition, (which tells us the status of the variable, confirmed, false positive, or candidate) koi score, 4 koi flags (4 tests to determine validity of planet, planetary data, and features of the star the planet revolves around. The dataset is generated by the Kepler telescope, which observes the data by looking at a star, and measuring the changes in brightness as an object moves between the star and the telescope.

We thought of many different questions and performed supervised and unsupervised learning to the dataset in order to find a solution to our questions. 

"The Data Science Process is about observation, model building, analysis and conclusion"
We thoroughly followed the data science process as shown below: 
1. Ask questions and identify the problem
2. Data Collection
3. Data Exploration
4. Data Modeling
5. Data Analysis
6. Visualization and Presentation of Result


# Step 1: Ask questions and identify the problem

After looking at the data, we thought of many different questions and the problems we wanted to tackle. We seperated them into three categories depending on the technique that could be applied to the problem: exploratory data analysis (EDA), supervised learning, and unsuperised learning. 

Exploratory Data Analysis

These questions are answered by basic visualizations and/or descriptive statistics.

1) Are binary stars more likely to host planets?
2) What are the feature distributions of likely habitable planets?
3) What does sky-projected distance represent?
4) What do the stars of Earth-like planets look like, and how do they compare to our sun?
5) Do the Earth-like planets congregate within certain patches of the night sky?

Supervised Learning

These questions invoke the use of supervised learning techniques to develop predicitve models.

1) Can we determine a classification system for exoplanet candidates (koi_disposition)? 

Unsupervised Learning 

These questions, or problems, invoke the use of unsupervised learning techniques to devise labels for observations.

1) K-Means Clustering: Planet Categorization

# Step 2: Some Quick Visualizations and Exploratory Analysis 

## Exploratory Data Analysis

```{r}

starData <- read.csv("cumulative.csv", header = TRUE, na.strings = "")
kepler_df <- starData
head(starData)
```

Load in all the required libraries.

```{r}
library(Amelia)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(rpart)
library(rpart.plot)
library(corrplot)
library(plotly)
library(rpart)
library(randomForest)
library(class)
library(e1071)
library(neuralnet)
```

First, we analyzed the relationship between koi_disposition and koi_pdisposition and found the similarities/differences.

```{r}
levels(starData$koi_disposition)
levels(starData$koi_pdisposition)
```

koi_disposition has two categories and koi_pdisposition has three categories. The two dispositions share two of the same categories,"CANDIDATE" & "FALSE POSITIVE"  as koi_disposition has an additional category named "CONFIRMED"

```{r}
Data1 <- subset(starData, koi_disposition =="CONFIRMED" & koi_pdisposition =="CANDIDATE")

```
```{r}
Data2 <- subset(starData, koi_disposition =="CONFIRMED" & koi_pdisposition =="FALSE POSITIVE")
 
```
```{r}
Data3 <- subset(starData, koi_disposition =="CANDIDATE" & koi_pdisposition =="CANDIDATE")


```
```{r}
Data4 <- subset(starData, koi_disposition =="CANDIDATE" & koi_pdisposition =="FALSE POSITIVE")

```
```{r}
Data5 <- subset(starData, koi_disposition =="FALSE POSITIVE" & koi_pdisposition =="CANDIDATE")
```
```{r}
Data6 <- subset(starData, koi_disposition =="FALSE POSITIVE" & koi_pdisposition =="FALSE POSITIVE")
```


Out of 9564 rows, 
When koi_disposition is classified as Confirmed
  -> koi_pdisposition classified the instances as Candidate 2248 times
  -> koi_pdisposition classified the instances as False Positive 45 times 
    -> misclassification rate of 2%
  
  
When koi_disposition is classified as Candidate
  -> koi_pdisposition classified the instances as Candidate 2248 times
  -> koi_pdisposition classified the instances as False Positive 0 times 
    -> misclassification rate of 0%
  
  
When koi_disposition is classified as False Positive 
  -> koi_pdisposition classified the instances as Candidate 0 times
  -> koi_pdisposition classified the instances as False Positive 5023 times 
    -> misclassification rate of 0%

As seen in our analysis, the koi_disposition and koi_pdisposition are very similar. There are small discrepancies which are only present when koi_disposition is classified as Confirmed.  

Plotting histogram of the koi_score to get a better understanding of the koi_score
```{r}
hist(starData$koi_score)
```
As seen in the histogram above, the most frequent scores are located at 0 and 1. The other koi_scores make up a small percentage of the results and can be rounded to the nearest integer: either 0 or 1. 

Analyzing the different column names of the Kepler dataset 
```{r}
titleLabels <- names(starData)
titleLabels
```

First, obtain a visual of the blank/missing/empty data:

```{r}
head(kepler_df)
summary(kepler_df)
```

```{r}
missmap(kepler_df)
```

Let's get rid of the error measurements that are fully blank. Let's also get rid of some obviously useless features:

```{r}
kepler_df$koi_teq_err1 <- NULL
kepler_df$koi_teq_err2 <- NULL
kepler_df$rowid <- NULL
kepler_df$kepid <- NULL
```

We should note that koi_teq_err1 and koi_teq_err2, which were deleted, quantify the error margin for the effective temperature for planets.

There are still a fair number of "NA" values remaining, but the data frame is workable from here.

```{r}
missmap(kepler_df)
```

Let's generate some summary stats:

```{r}
summary(kepler_df)
```

Summary information on the structure of the dataframe:

```{r}
str(kepler_df)
```

We see there are many numeric values, some of which pertain to esoteric astronomical measures. Part of the ensuing investigation into the data will be to test our understanding of what certain features mean.

```{r}
# First extract only those rows where the planet is named.
named_planets_df <- subset(kepler_df, !is.na(kepler_name))
```

```{r}
missmap(named_planets_df)
summary(named_planets_df$koi_teq)
summary(named_planets_df$koi_prad)
```

## EDA Question #1: Are binary stars more likely to host planets?

There is a "false positive flag" associated with the likely presence of a binary star (i.e. it is set to '1' if the observed light curve is likely due to a binary star). We want to use these flags to determine what proportion of candidate planets are found around (probable) binary stars. We also want to compare what the literature says regarding the planetary hosting ability of binary stars to what the Kepler analysis suggests.

```{r}
# Let's create labels for the binary star false positive flag
for (i in 1:dim(kepler_df)[1]) {
  if (kepler_df$koi_fpflag_ss[i] == 0)
    kepler_df$koi_fpflag_ss[i] <- "No binary star detected"
  else
    kepler_df$koi_fpflag_ss[i] <- "Probable binary star"
}

# Plot the dispositions according to Kepler data analysis
plot1 <- ggplot(kepler_df, aes(x = koi_fpflag_ss, fill = koi_pdisposition)) + geom_bar(position = "fill")
plot2 <- ggplot(kepler_df, aes(x = koi_fpflag_ss, fill = koi_disposition)) + geom_bar(position = "fill")
plot1
plot2
```

The above plots show that binary stars have a much smaller proportion of likely planets encircling them than do single stars, and this is seen in both the Kepler analysis labels and literature labels. But this may not reflect the actual capability of binary stars to host planets. These plots could just be a reflection of how difficult it is to detect planets encircling binary star systems.

## EDA Question #2: What are the feature distributions of likely habitable planets?

Using the data available we can filter planets according to their likeness to Earth and treat these planets as likely being "habitable". Although, we should note that the filtering is very crude. The only two criteria we can rely on to filter planets are effective temperature (koi_teq) and radius (koi_prad). Using only these two criteria are not enough to determine the habitableness of a planet. Other data regarding planet composition, for example, are needed to make definitive judgement on habitability. But we'll proceed with the crude method for the purpose of this analysis.

We'll take the habitable planets to be approximately Earth size and within a temperature range to support liquid water on the surface. Also, we'll only look at planets with decent koi_score values (where koi_score is a measure of how certain scientists are the corresponding observation is a planet).

```{r}
habit_df <- subset(kepler_df, koi_prad >= 0.5 & koi_prad <= 2.0 & koi_teq >= 273 & koi_teq <= 373 & koi_pdisposition == "CANDIDATE" & koi_score >= 0.4)
str(habit_df)
summary(habit_df)
head(habit_df)
```

Now let's visualize the features of the "haitable" planets.

```{r}
 # Temperature distribution
ggplot(habit_df, aes(x = koi_teq, fill = koi_disposition)) + geom_bar(binwidth = 9) + xlab("Effective Temperature (Kelvin)") + labs(title = "Temperature Distribution of Likely Habitable Planets\n") + geom_vline(xintercept=252, colour="orange", linetype = "longdash") + annotate("text", x = 267, y = 6, label = "Effective Temp\nof Earth")
```

Observation: it appears that all the Earth-like planets in the dataset are considerably warmer than the Earth.

```{r}
# Radius distribution
ggplot(habit_df, aes(x = koi_prad, fill = koi_disposition)) + geom_bar(binwidth = 0.1) + xlab("Planetary Radius (Earth Radii)") + labs(title = "Planetary Radius Distribution of Likely Habitable Planets\n") + geom_vline(xintercept=1, colour="orange", linetype = "longdash") + annotate("text", x = 1.14, y = 6, label = "Earth Radius")
```

Observation: most of the Earth-like planets in the dataset are substantially larger than the Earth.

Let's investigage sky-projected distances.

```{r}
# koi_impact distribution
ggplot(habit_df, aes(x = koi_impact, fill = koi_disposition)) + geom_bar(binwidth = 0.1) + xlab("Sky-Projected Distance") + labs(title = "Sky-Projected Distance Distribution of Likely Habitable Planets\n")
```

Observation: The distribution looked negative exponential, and suggested Earth-like planets tended to have smaller sky-projected distances. We were not certain if the negative exponential shape lended itself to any special interpretation, or if koi_impact measures related to any sort of Poisson point process. Such an interpretation was especially difficult to make seeing that we did not really know what sky-projected distance represented.

We did suspect, however, that sky-projected distance was a proxy for the actual distance between a planet and its star. This suspicion arose out of the fact that the "goldilocks" zone for a planet tended to be closer to the star. Therefore we expected habitable planets to be located somewhat closer to stars. A continuation of the investigation into sky-projected distance is detailed below...

But first we finish detailing our analysis of likely habitable planets. Here is a scatterplot of the Earth-like planets.

```{r}
ggplot(habit_df, aes(x = koi_prad, y = koi_teq, size = koi_score)) + geom_point(aes(color = koi_disposition)) + labs(title = "Plot of Likely Habitable Planets\n") + xlab("Planetary Radius (Earth Radii)") + ylab("Effective Temperature (Kelvin)")
```

Observation: There's no information here that was not revealed above. The likely habitable planets in the dataset were typically larger and warmer than the Earth.

## EDA Question #3: What does sky-projected distance represent?

Now let's proceed to better understand sky-projected distance. We had a hypothesis that sky-projected distance was a proxy for actual distance from a star. Luckily, we had planet features that related to the orbital speeds of planets. The laws of physics dictate that planets further out from a star are generally slower moving. So, by plotting sky-projected distance in relation to transit duration, or period between transits, we could see if sky-projected distance increased with larger transit duration.

But first, we examine the distribution of sky-projected distances for all planets.

```{r}
# Let's take a dataset where we have sky-projected distance data, where the koi_score is fairly high, and where there is no FLASE POSITIVE label. The motivation for this is to remove any erroneous data associated with observations that are not likely to be planets.
test_df <- subset(kepler_df, !is.na(koi_impact) & koi_score > 0.5 & koi_disposition != "FALSE POSITIVE" & koi_disposition != "FALSE POSITIVE" & koi_impact <= 1.0)
```

```{r}
ggplot(test_df, aes(x = koi_impact, fill = koi_disposition)) + geom_histogram(binwidth = 0.01) + xlab("Sky-Projected Distance") + labs(title = "Sky-Projected Distance Distribution of Likely Planets\n") + xlim(c(0,1))
summary(test_df$koi_impact)
print(mean(test_df$koi_impact))
print(sd(test_df$koi_impact))
```

As with the likely habitable planets, we see a negative exponential-looking distributiion for sky-projected distance. Therefore the hypothesis mentioned above is shown to be false: it's not just habitable planets that tend to have smaller koi_impact. Planets, in general, are likely to have small koi_impact values. This puts koi_impact into doubt as a predictor/proxy for actual distance of a planet from a star.


## EDA Question #4: What do the stars of Earth-like planets look like, and how do they compare to our sun?

Then we turn our attention to the stars that host Earth-like planets. How did they compare to our Sun?

We plot the star radii v. their photospheric temperature.

```{r}
temp_df <- habit_df[,c("koi_steff","koi_slogg","koi_srad")]
sun <- c(5778, 2.43775056282, 1.00)
temp_df[dim(temp_df)[1]+1,] <- sun


ggplot(temp_df, aes(x = koi_srad, y = koi_steff)) + geom_point(aes(colour = koi_slogg), size = 5) + scale_colour_gradient2(low = "#FF3300", mid = "white", high = "#663300", midpoint = 3.6) + xlab("Photospheric Radius of the Star (Normalized to Sun's Radius)") + ylab("Photospheric Temperature of the Star (Kelvin)") + labs(title = "Stars with Potentially Habitable Planets\n", color = "Base-10 Log\nof Surface Gravity\nAcceleration") + annotate("text", x = 1.06, y = 5778, label = "Sun") + geom_smooth(method='lm',formula=y~x) + annotate("text", x = 0.5, y = 6000, label = paste("R-square =", round(cor(temp_df$koi_srad,temp_df$koi_steff)^2, digits = 4)))

# Print the correlation
print(cor(temp_df$koi_srad,temp_df$koi_steff))
```

Observation: Most of the stars that host Earth-like planets seemed to be smaller, cooler, and have larger surface accelerations when comapred to our Sun. What was also interesting to see was the fairly high degreee of correlation observed between photospheric radius and temperature of stars hosting Earth-like planets. The correlation table above, built for all candidate planets with a fairly high koi_score, did not suggest this would be the case. But when we look at the subset of stars that host Earth-like planets, the relation is apparent. It's not clear as to why this is the case, however.

## EDA Question #5: Do the Earth-like planets congregate within certain patches of the night sky?

A Kaggle user used right ascension and declination data, the celestial coordinates of the observations, to see where candidates and confirmed planets were being observed. We wanted to do something similar, but use the resulting plot in a different way: to see if Earth-like planets were restricted to certain patches of the night sky.

Add labels to the overall dataset: "Earth-like" and "Not Earth-like".

```{r}
for (i in 1:dim(kepler_df)[1]) {
  # Do a check for NAs
    na_check <- is.na(kepler_df$koi_prad[i]) | is.na(kepler_df$koi_teq[i]) | is.na(kepler_df$koi_score[i]) | is.na(kepler_df$koi_pdisposition[i])
  if (!na_check) {
    if (kepler_df$koi_prad[i] >= 0.5 & kepler_df$koi_prad[i] <= 2.0 & kepler_df$koi_teq[i] >= 273 & kepler_df$koi_teq[i] <= 373 & kepler_df$koi_pdisposition[i] == "CANDIDATE" & kepler_df$koi_score[i] >= 0.4) {
      kepler_df$koi_els[i] <- "Earth-like"
    } else {
      kepler_df$koi_els[i] <- "Not Earth-like"
    }
  }
}
```

Now we plot celestial coordinate data overlaid with with the new koi_els feature information.

```{r}
el_df <- subset(kepler_df, koi_els == "Earth-like")

ggplot(kepler_df, aes(x = ra, y = dec)) + geom_point(aes(colour = koi_els), size = 1.5) + xlab("Right Ascension") + ylab("Declination") + labs(title = "Celestial Positioning of Observations\n") 
```

As suspected, the observations corresponding to Earth-like planets are spread out accross the patches of celestial coordinates observed by Kepler. Another hypothesis was that perhaps the patches with a higher density of observations would also have more observations of Earth-like planets. This does not seem to be the case, however.

# Step 3: Data Exploration 

## Cleaning Data 

```{r}

starData <- read.csv("cumulative.csv", header = TRUE)
head(starData)
```


Assigning NA to the blank values in the entire dataset

```{r}
starData[starData ==""] <- NA
```


List the name and number of the columns that have at least one missing value. 

```{r}

naCol <-  which(colMeans(is.na(starData))>0) 
naCol
```


```{r}
naVal<- vector()

for (colnum in 1:50) {
  
  naVal[colnum] <- sum(complete.cases(starData[colnum])==FALSE) 
  
}


```


```{r}
naVal<-naVal[naVal!=0]
```

```{r}
NaData <- data.frame(naCol,naVal)
```

```{r}
barplot(NaData$naVal,main = "Missing Value Counts", names.arg = NaData$naCol, cex.names = 0.5,
  	xlab="column names", col="red")

```
```{r}
titleLabels[4]
titleLabels[7]
titleLabels[31]
titleLabels[32]
```
As shown above, the columns/features with the most missing values are ranked as follows:
1."koi_teq_err1" & "koi_teq_err2" 
2."kepler_name"
3."koi_score"

Since there are many occurences of missing values in many of the columns, it is unreasonable to delete all the missing value data. Disregarding the top 4 columns with the most missing values, there seems to be 10% of missing values for many of the columns. If we were to delete all the rows with at least one missing data, we wouldn't be removing 10% of the data, it would be close to 50% since the missing values are not all located in the same rows. 

Instead of remvoing all the rows with missing values, we did two different things: insert the mean to the missing numerical values and insert the median to the missing categorical data. 

```{r}
starData<- starData[-c(4,7,31,32)]

```
```{r}
levels(starData$koi_tce_delivname)
```
```{r}
levels(starData$koi_tce_delivname)
```

```{r}
dataA <- subset(starData,starData$koi_tce_delivname =="q1_q16_tce")
dataB <- subset(starData,starData$koi_tce_delivname =="q1_q17_dr24_tce")
dataC <- subset(starData,starData$koi_tce_delivname =="q1_q17_dr25_tce")
dim(dataA)[1]
dim(dataB)[1]
dim(dataC)[1]
```

```{r}
starData$koi_tce_delivname[is.na(starData$koi_tce_delivname)] <- "q1_q17_dr25_tce"
starData$koi_tce_delivname <- factor(starData$koi_tce_delivname)
levels(starData$koi_tce_delivname)
```


```{r}
starData$koi_tce_plnt_num <- as.factor(starData$koi_tce_plnt_num)

levels(starData$koi_tce_plnt_num)
```


```{r}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

Mode(starData$koi_tce_plnt_num)
```
```{r}
starData$koi_tce_plnt_num[is.na(starData$koi_tce_plnt_num)] <- "1"
```


```{r}
for(i in 6:32){
  starData
  starData[is.na(starData[,i]), i] <- mean(starData[,i], na.rm = TRUE)

}
```

```{r}
for(i in 35:46){
  starData
  starData[is.na(starData[,i]), i] <- mean(starData[,i], na.rm = TRUE)

}
```

```{r}
sum(complete.cases(starData)=="FALSE")
```

```{r}
starData <- starData[-c(1:3,5)]
starData$koi_fpflag_co <- as.factor(starData$koi_fpflag_co)
starData$koi_fpflag_ec <- as.factor(starData$koi_fpflag_ec)
starData$koi_fpflag_nt <- as.factor(starData$koi_fpflag_nt)
starData$koi_fpflag_ss <- as.factor(starData$koi_fpflag_ss)
starData$koi_tce_delivname <- as.factor(starData$koi_tce_delivname)
starData$koi_tce_plnt_num <- as.factor(starData$koi_tce_plnt_num)
```


Rearrange Dataset that it seperates categorical variables and numerical variables

```{r}
starData <- starData[c(1:5,29,30,6:28,31:42)]
head(starData)
```

# Step 4 - 6: Data Modelling, Data Analysis, and Data Visualization

## Create randomized training and testing set

```{r}
num_samples = dim(starData)[1]
sampling.rate = 0.8
training <- sample(1:num_samples, sampling.rate * num_samples, replace=FALSE) 
trainingSet <- subset(starData[training, ])
testing <- setdiff(1:num_samples,training)
testingSet <- subset(starData[testing, ])
```

```{r}
names(trainingSet)
```

## Supervised Learning 

### 1) Can we determine the classification system for exoplanet candidates (koi_disposition)? 

The koi_disposition has three different categories as mentioned above: "CANDIDATE"," "CONFIRMED", "FALSE POSITIVE". Blank values are classified as "NOT DISPOSITIONED" which will be ignored. These are the results from historical dispositions in literature for exoplanet candidates. KOI means Kepler's "object of interest" which is comprised of the planets that Kepler has found. The objective of the model is to predict whether a KOI is a candidate, confirmed or false positive. 

For this problem we conducted the following data science models: 
- Decision Tree
- Randomforest
- KNN 
- SVM
- Neural Network
Linear regression was ignored since there were many categorical variables.
Logistic regression was ignored since there are three different categories of koi_disposition which makes it complex to analyze. 

### Decision Tree 

```{r}
decTreeModel <- rpart(koi_disposition ~ .,data=trainingSet,method = "class")
prp(decTreeModel)
```


```{r}
plotcp(decTreeModel)
```

```{r}
pruned_decTreeModel = prune(decTreeModel, cp=0.012)
prp(pruned_decTreeModel)
```
As shown above, the most imporant factors and characteristics in determining the classification of a star is ranked as follows:
1. koi_fpflag_s
2. koi_fpflag_n
3. koi_fpflag_C
4. koi_model_sn
5. koi_fpflag_e
6. koi_prad_err 
The decision tree makes it very easy to understand and visualize the important aspects in this problem. It was very first fast to implement as it can handle both categorical and numerical data. 
```{r}
predictedLabels<-predict(pruned_decTreeModel, testingSet, type = "class")
```

```{r}
sizeTestSet = dim(testingSet)[1]
error = sum(predictedLabels != testingSet$koi_disposition)
misclassification_rate = error/sizeTestSet
print(misclassification_rate)
```

### Random Forest

```{r}
RandForestModel <- randomForest(koi_disposition ~ .,data=trainingSet)
```


```{r}
plot(RandForestModel)
legend("top", colnames(RandForestModel$err.rate),fill=1:3)
```

```{r}
predictedLabels<-predict(RandForestModel, testingSet)
sizeTestSet = dim(testingSet)[1]

error = sum(predictedLabels != testingSet$koi_disposition)

misclassification_rate = error/sizeTestSet

print(misclassification_rate)
```

The randomforest model had a lower misclassification rate than the decision tree. Decision trees are prone to overfitting. Randomforest models mititages overfitting and can lead to more accurate classification and prediction which is seen in this case. 

### KNN Model

Normalize all data

```{r}
starData[8:42] <- scale(starData[8:42])
```

change koi_tce_delivname into numerical values

```{r}
levels(starData$koi_tce_delivname)
```
```{r}
levels(starData$koi_tce_delivname)[levels(starData$koi_tce_delivname)=="q1_q16_tce"] <- "1"
levels(starData$koi_tce_delivname)[levels(starData$koi_tce_delivname)=="q1_q17_dr24_tce"] <- "2"
levels(starData$koi_tce_delivname)[levels(starData$koi_tce_delivname)=="q1_q17_dr25_tce"] <- "3"


starData$koi_tce_delivname[starData$koi_tce_delivname== "q1_q16_tce"] <- "1"
starData$koi_tce_delivname[starData$koi_tce_delivname== "q1_q17_dr24_tce"] <- "2"
starData$koi_tce_delivname[starData$koi_tce_delivname== "q1_q17_dr25_tce"] <- "3"
levels(starData$koi_tce_delivname)
```

```{r}
num_samples = dim(starData)[1]
sampling.rate = 0.8
training <- sample(1:num_samples, sampling.rate * num_samples, replace=FALSE)
trainingSet <- starData[training, ]
testing <- setdiff(1:num_samples,training)
testingSet <- starData[testing, ]
```

```{r}
trainingfeatures <- subset(trainingSet, select=c(-koi_disposition))

traininglabels <- trainingSet$koi_disposition

testingfeatures <- subset(testingSet, select=c(-koi_disposition))
```

```{r}
currentBestError = Inf
currentBestVar = -1
for(i in 1:30) { 
  predictedLabels = knn(trainingfeatures,testingfeatures,traininglabels,k=i)
  error = sum(predictedLabels != testingSet$koi_disposition)
  if(error < currentBestError){
    print(paste0("We found a better k: ",i))
    currentBestError = error 
    currentBestVar = i
  }
}
```


```{r}
currentBestVar
```

```{r}
currentBestError / (dim(testingSet)[1])
```

### KNN cross fold validation 

```{r}
AllErrors=c()
for(fold in 1:50)
{
  #Get Training at Testing sets
  num_samples = dim(starData)[1]
  sampling.rate = 0.8
  training <- sample(1:num_samples, sampling.rate * num_samples, replace=FALSE)
  trainingSet <- starData[training, ]
  testing <- setdiff(1:num_samples,training)
  testingSet <- starData[testing, ]
  
  trainingfeatures <- subset(trainingSet, select=c(-koi_disposition))

  traininglabels <- trainingSet$koi_disposition

  testingfeatures <- subset(testingSet, select=c(-koi_disposition))
 
  predictedLabels = knn(trainingfeatures,testingfeatures,traininglabels,k=currentBestVar)
    
  
  error = sum(predictedLabels != testingSet$koi_disposition)
  errorRate <- error / (dim(testingSet)[1])
  AllErrors[fold] = errorRate
}

```

```{r}
AverageError = mean(AllErrors)
AverageError
```
By conducting a KNN cross validation analysis, we are able to find the average error which is a more accurate result than doing one test. 

### SVM

```{r}
starData$koi_disposition <- as.factor(starData$koi_disposition)
levels(starData$koi_disposition)
```

### SVM Linear 

This model takes a long calculation time, so insert  print(i) to check the progress
ideally, we would increase the range of cost testing, but to consider the process time, we choose 15-20 to demonstrate the concept. Feel free to edit the range for for-loop during assessment

```{r}
currentBestError = Inf
currentBestVar = -1
for(i in 15:20) {
  svmModel <- svm(koi_disposition~., data=trainingSet, kernel="linear", cost=i)
  error = sum(predictedLabels != testingSet$koi_disposition)
  print(i)
  if(error < currentBestError){
    print(paste0("We found a better cost: ",i))
    currentBestError = error 
    currentBestVar = i
  }
}
```

```{r}
currentBestVar
currentBestError / (dim(testingSet)[1])
```


```{r}
currentBestError = Inf
currentBestVar = -1
for(i in 15:20) {
  svmModel <- svm(koi_disposition~., data=trainingSet, kernel="polynomial", cost=i)
  error = sum(predictedLabels != testingSet$koi_disposition)
  print(i)
  if(error < currentBestError){
    print(paste0("We found a better cost: ",i))
    currentBestError = error 
    currentBestVar = i
  }
}
```

```{r}
currentBestVar
currentBestError / (dim(testingSet)[1])
```

```{r}
currentBestError = Inf
currentBestVar = -1
for(i in 15:20) {
  svmModel <- svm(koi_disposition~., data=trainingSet, kernel="radial", cost=i)
  error = sum(predictedLabels != testingSet$koi_disposition)
  print(i)
  if(error < currentBestError){
    print(paste0("We found a better cost: ",i))
    currentBestError = error 
    currentBestVar = i
  }
}
```

```{r}
currentBestVar
currentBestError / (dim(testingSet)[1])
```

### Neural Network

Using a more complex machine learning algorithm, 
```{r}
head(starData)
```


```{r}
levels(starData$koi_disposition)
```

```{r}
levels(starData$koi_disposition)[levels(starData$koi_disposition)=="CANDIDATE"] <- "1"
levels(starData$koi_disposition)[levels(starData$koi_disposition)== "CONFIRMED"] <- "2"
levels(starData$koi_disposition)[levels(starData$koi_disposition)== "FALSE POSITIVE"] <- "3"

starData$koi_disposition[starData$koi_disposition=="CANDIDATE"] <- "1"
starData$koi_disposition[starData$koi_disposition== "CONFIRMED"] <- "2"
starData$koi_disposition[starData$koi_disposition== "FALSE POSITIVE"] <- "3"
starData$koi_disposition <- as.numeric(starData$koi_disposition)
```

```{r}
str(starData)
```

```{r}
starData$koi_fpflag_nt <- as.numeric(starData$koi_fpflag_nt)
```

```{r}
starData$koi_fpflag_nt <- as.numeric(starData$koi_fpflag_nt)
starData$koi_fpflag_co <- as.numeric(starData$koi_fpflag_co)
starData$koi_fpflag_ec <- as.numeric(starData$koi_fpflag_ec)
starData$koi_fpflag_ss <- as.numeric(starData$koi_fpflag_ss)
starData$koi_tce_delivname <- as.numeric(starData$koi_tce_delivname)
starData$koi_tce_plnt_num <- as.numeric(starData$koi_tce_plnt_num)
str(starData)
```

```{r}
num_samples = dim(starData)[1]
sampling.rate = 0.8
training <- sample(1:num_samples, sampling.rate * num_samples, replace=FALSE) 
trainingSet <- subset(starData[training, ])
testing <- setdiff(1:num_samples,training)
testingSet <- subset(starData[testing, ])
```

```{r}
n <- names(starData)
f <- as.formula(paste("koi_disposition ~", paste(n[!n %in% "koi_disposition"], collapse = " + ")))
f
```

```{r}
nnModel <- neuralnet(f, data=trainingSet, hidden=c(7,5,3), linear.output=FALSE)
plot(nnModel)
```

```{r}
predictedLabels <-compute(nnModel, testingSet[,2:42])
predictedLabels<-round(predictedLabels$net.result)
sizeTestSet = dim(testingSet)[1]
error = sum(predictedLabels != testingSet$koi_disposition)
misclassification_rate = error/sizeTestSet
print(misclassification_rate)
```

After conducting all of the models, the most accurate model is the randomforest model since it had the lowest misclassification rate of approximately 11%. 


# Unsupervised Learning 

## k-Means Clustering: Planet Categorization

One of the more major investigations we conducted was for an unsupervised learning problem: Given the planet data available, could we use unsupervised learning methods to come up with some planet categorization scheme? And, assuming we have a viable scheme, does our categorization scheme match any of those which astronomers have already devised?

The tool we wanted to use was k-means clustering. We first needed to decide which planets we would like to perform clustering on. and settled on planets with a koi_score of at least 0.8, because we wanted to be reasonably certain that the observations we included were planets. This still left us with a decent number of datapoints to work with.

```{r}
# Verify there is an adequate volume of data after proposed koi_score filtering.
num_pts <- sum(kepler_df$koi_score >= 0.8 & !is.na(kepler_df$koi_score))
ggplot(kepler_df, aes(x = koi_score, fill = koi_pdisposition)) + geom_histogram() + geom_vline(xintercept=0.8, colour="orange", linetype = "longdash") + annotate("text", x = 0.7, y = 2000, label = "koi_score\ncutoff")
print(paste("The number of data points for koi_score >= 0.8:", num_pts))
```


```{r}
planets_df <- subset(kepler_df, koi_score >= 0.8)
head(planets_df)
str(planets_df)
```

For clustering we needed to narrow down the appropriate feature set. Three criteria were used to narrow down the feature set:
1. Ignore non-numeric features (since we'll use Euclidean distances for clustering).
2. Ignore data that have no relation to the physical features of the planet.
3. Ignore redundant data.

```{r}
# Vector of features to include
keep <- c("koi_period","koi_impact","koi_duration","koi_depth","koi_prad","koi_teq")
# Create dataframe for k-means
plnts_clst_df <- planets_df[,keep]
plnts_clst_df$koi_teq <- as.numeric(plnts_clst_df$koi_teq)
# Remove "NA" rows
plnts_clst_df <- subset(plnts_clst_df, !is.na(koi_teq))
head(plnts_clst_df)
str(plnts_clst_df)
```

Feature normalization:

```{r}
# Normalize using z scores:
norm_plnts_clst_df <- plnts_clst_df
for (i in 1:dim(norm_plnts_clst_df)[2]) {
  norm_plnts_clst_df[,i] <- (norm_plnts_clst_df[,i]-mean(norm_plnts_clst_df[,i]))/sd(norm_plnts_clst_df[,i])
}
head(norm_plnts_clst_df)
```

```{r}
totalWithnss = c()
betweenss = c()
withinss <- c()
# Use k folds to acheive stability because centroids are selected randomly
for(clusters in 2:80)
{
  fit <- kmeans(norm_plnts_clst_df, clusters) 
  totalWithnss[clusters] <- fit$tot.withinss
  betweenss[clusters] <- fit$betweenss
}

plot(totalWithnss)
plot(betweenss)
plot(totalWithnss/betweenss)
```

The "totalWithnss", "betweenss", and "totalWithnss/betweenss" plots seemed to suggest that k = 20 was a good choice for the number of starting centroids and, therefore, the number of categories present.

```{r}
# Apply the appropriate number of categories.
fit <- kmeans(norm_plnts_clst_df, 20)
plnts_clst_df$Category <- fit$cluster
norm_plnts_clst_df$Category <- fit$cluster
plnts_clst_df$Category <- as.factor(plnts_clst_df$Category)
norm_plnts_clst_df$Category <- as.factor(norm_plnts_clst_df$Category)
```

Now that all observations were associated with one of twenty categories, according to k-means, we tried to understand which features are responsible for the most differentiation between the categories present.

```{r}
# create a daraframe that excludes the categories
pca_plnts_clst_df <- plnts_clst_df[1:50,1:6]
pca_model <- prcomp(pca_plnts_clst_df, center = TRUE, scale. = TRUE)
biplot(pca_model)
```

The principal component graph suggested that koi_teq and koi_period were very useful for desdcribing differentiation between categories. We also see that koi_prad, koi_imapct, and koi_depth were vectors that essentially caused separation of clusters in the same direction. So perhaps only one of them was needed. The same was said about koi_duraiton and koi_period.

To bolster this analysis (i.e. finding the features most repsonsible for cluster separation), we used a decision tree to reveal the most important features that determined accurate categorizaiton.

We'll use the "plnts_clst_df" data frame to build the decision tree.

```{r}
dt_model <- rpart(data = plnts_clst_df, Category ~.)
plotcp(dt_model)
prp(dt_model)

pruned_dt_model <- prune(dt_model, cp = 0.023)
prp(pruned_dt_model)
```

The tree suggested that koi_impact, koi_teq, and koi_duration were the most important features. These results seemed to agree with the principal component analysis above.

We then created histograms that showed the distributions of categories accross all the features taken for the planets.

```{r}
ggplot(plnts_clst_df, aes(x = koi_teq, fill = Category)) + geom_histogram(binwidth = 50) + xlim(c(0,4000))
ggplot(plnts_clst_df, aes(x = koi_impact, fill = Category)) + geom_histogram(binwidth = 0.01)
ggplot(plnts_clst_df, aes(x = koi_prad, fill = Category)) + geom_histogram(binwidth = 0.2) + xlim(c(0,10))
ggplot(plnts_clst_df, aes(x = koi_depth, fill = Category)) + geom_histogram(binwidth = 20) + xlim(c(0,5000))
ggplot(plnts_clst_df, aes(x = koi_duration, fill = Category)) + geom_histogram(binwidth = 0.2) + xlim(c(0,20))
ggplot(plnts_clst_df, aes(x = koi_period, fill = Category)) + geom_histogram(binwidth = 1) + xlim(c(0,100))

```

Scanning the histograms of koi_teq, koi_impact, and koi_duration showed certain ranges in which some categories were prevalent and others are not. That is to say, there were bounds of separation between categories within these features which seemed to be a pretty good foundation for a feature space.

We created some scatter plots to enhance the visual analysis.

```{r}
ggplot(plnts_clst_df, aes(x = koi_impact, y = koi_teq, color = Category)) + geom_point() + ylim(c(0,2000)) + xlim(c(0,1.5))
ggplot(plnts_clst_df, aes(x = koi_impact, y = koi_duration, color = Category)) + geom_point() + ylim(c(0,10)) + xlim(c(0,1.5))
ggplot(plnts_clst_df, aes(x = koi_teq, y = koi_duration, color = Category)) + geom_point() + ylim(c(0,10)) + xlim(c(0,4000))
```

Since we had three principal features constituting the feature space, we tried a 3D plot.

```{r}
plot_3d_plnts_clst_df <- subset(plnts_clst_df, koi_teq <= 2000 & koi_impact >= 0 & koi_impact <= 1.5 & koi_duration <= 20)
plot_ly(plot_3d_plnts_clst_df, x = ~koi_impact, y = ~koi_teq, z = ~koi_duration, color = ~Category) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'Sky-Projected Distance'),
                     yaxis = list(title = 'Effective Temperature (K)'),
                     zaxis = list(title = 'Duration of Transit (Hours)')))
```

The three-dimension plot showed the clusters much more clearly than any of the 2D feature spaces built above. Although the clustering was not perfect due to the fact we chose to ignore several features, the 3D plot did confirm the usefullness of using koi_impact, koi_teq, and koi_duration to explain most clustering between points.

The final step in this clustering analysis was to segment the categorized planet dataset according to categories, then look at the feature distributions for koi_impact, koi_teq, and koi_duration for each category.

```{r}
# Subset dataframe on a categorical basis
#for (i in 1:20) {
#  temp_df2 <- subset(plnts_clst_df, Category == i)
#  for (j in 1:(dim(plnts_clst_df)[2]-1)) {
#    result.mean <- mean(plnts_clst_df[,names(plnts_clst_df)[j]], na.rm = TRUE)
#    result.median <- median(plnts_clst_df[,names(plnts_clst_df)[j]], na.rm = TRUE)
#    result.sd <- sd(plnts_clst_df[,names(plnts_clst_df)[j]], na.rm = TRUE)
#    result.max <- max(plnts_clst_df[,names(plnts_clst_df)[j]], na.rm = TRUE)
#    result.min <- min(plnts_clst_df[,names(plnts_clst_df)[j]], na.rm = TRUE)
    
    # Print the results
#    print(paste("The results for",names(plnts_clst_df)[j],"of category",i,"are..."))
#    print(paste("The mean:",result.mean))
#   print(paste("The median:",result.median))
#    print(paste("The std. dev.:",result.sd))
#    print(paste("The max:",result.max))
#    print(paste("The min:",result.min))
#    print("* * *")
#  }
#}
```

We found it helpful to create the following plots.

```{r}
for (i in 1:(dim(plnts_clst_df)[2]-1)){
  plot <- ggplot(plnts_clst_df, aes(x = plnts_clst_df[i], y = Category, color = Category)) + geom_point() + xlab(names(plnts_clst_df)[i]) +  theme(legend.position="none")
  print(plot)
}
```

It was difficult to make comments on the categorization scheme developed via k-means without thorough research being conducted on the features themselves and how they may be related to planet characteristics not explicitly found in the data. We should also note that one of the principal features -- koi_impact -- was not understood very well. So what this measure reveals about more tangible planet characteristics is still a mystery.

We'll conclude the unsupervised learning and analysis here and leave the in-depth research on the categories above for a later time.

# Conclusion 

After conducting EDA, supervised and unsupervised learning on the Kepler Dataset, we were able to learn a lot about this unfamiliar topic and formulate many conclusions. To reiterate, the questions we asked and key takeaways are shown below: 

## EDA

1) Are binary stars more likely to host planets?
2) What are the feature distributions of likely habitable planets?
3) What does sky-projected distance represent?
4) What do the stars of Earth-like planets look like, and how do they compare to our sun?
5) Do the Earth-like planets congregate within certain patches of the night sky?

When conductory an exploratory data anaylsis, we learned from Q1 that binary stars have a much smaller proportion of likely planets encircling them than do single stars, and this is seen in both the Kepler analysis labels and literature labels. From Q2, it appears that all the Earth-like planets in the dataset are considerably warmer than the Earth. From Q3, we noticed that sky-projected distance is very weakly, and essentially uncorrelated, with any other features of interest describing either the planet or the host star. From Q4, we realized that most of the stars that host Earth-like planets seem to be smaller, cooler, and have larger surface accelerations when comapred to our Sun. Last in EDA, we noticed that the observations corresponding to Earth-like planets are spread out accross the patches of celestial coordinates observed by Kepler.


## Supervised Learning

1) Can we determine the classification system for exoplanet candidates (koi_disposition)? 

For this question we conducted the following data science models and the misclassification rate was calculated for each model as follows:  
- Decision Tree: 12%
- Randomforest: 10%
- KNN: 23%
- SVM: 22%
- Neural Network: 78%
 
The RandomForest model was clearly the most accurate model with the lowest misclassification rate of 10%. From the decision tree, we noticed that the most important factors for determining the classification of a planet were the false positive flags. Hopefully, with this 10% classification rate, this data science model could save time and resources for NASA when they are confirming planets. 


## Unsupervised Learning 

1) K-Means Clustering: Planet Categorization

Through various means of clustering and analysis, we noticed a few strong characteristics when categorizing planets: koi_impact, koi_teq, and koi_duration. These variables were very useful for describing differentiation between categories. One major challenge we faced was being unfamiliar with this topic and our lack of knowledge limited our analysis especially in unsupervised learning. Without further research on planetary features, it was hard to draw conclusions. This is a key focus moving forward in order to enhance our unsupervised learning analysis. 


# Challenges Faced & Next Steps

It was a great challenge to work with a dataset without any prior knowledge of the characteristis and features of a planet. However, through the power of data science, we were able to discover hidden relationships and the most important factors when answering our questions for supervised and unsupervised learning. Moving forward, in order to improve our data model, we will focus on learning more about each feature and talk to an expert in this particular field to enhance data quality, recognize errors in the data, better data analysis and discover more relevant relationships & planet characteristics in unsuperised learning. 

As well, when we were predicting the classification of KOI's disposition, it was complex to perform a logistic regression since there were three categories. This is a model that will need to be explored and completed in the future. 

We also faced problems with neural networks. We inserted too many inputs into the network, which led to many unneccesary layers that confused the system leading to an incredibly high misclassification rate. This is something that will need to be simplified and explored further in order to grasp the great capabilities of neural networks. 




